train:
  max_updates: 300000
  val_every_updates: 1000
  log_every_updates: 200
  use_tqdm: true
  seed: 39
  deterministic: false
  compile:
    enabled: true
    mode: reduce-overhead  # options: default, reduce-overhead, max-autotune
    fullgraph: false
  # Reconstruction loss for Slot-JEPA: 'l1' or 'l2'
  reconstruction_loss: l1
  batch_size: 64
  num_workers: 8
  images_only: true
  return_properties: false
  max_samples_train: null
  max_samples_val: 1000
  max_val_batches: 15  # Only validate on first 15 batches for speed
  grad_clip_norm: 1.0
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  lr_schedule:
    type: cosine
    warmup_steps: 10000
    min_lr: 1.0e-6
  ckpt:
    every_updates: 5000
    keep_last: 3
    resume_path: null
    resume_latest: false

data:
  dataset: coco
  root: ./data/coco
  image_size: 256
  max_objects: 40
  train_split: train2017
  val_split: val2017
  min_area: 0.0
  train_horizontal_flip_prob: 0.5

dino:
  size: s
  freeze: true
  compile: false

slots:
  num_iterations: 3
  num_slots: 20
  num_heads: 1
  slot_size: 256
  input_size: null
  out_size: null
  mlp_hidden_size: 512
  detach_last_iteration: true
  qk_rmsnorm: false
  qk_rmsnorm_eps: 1.0e-6
  rope: {}

decoder:
  type: transformer
  num_heads: 8
  transformer:
    qk_norm: true
    depth: 4
    mlp_hidden_dim: 1024
    pre_mlp: true
    dropout: 0.15
    rope: {}
  mlp:
    hidden_dim: 1024
    depth: 2
    rope: {}

masking:
  ratio: 0.7
  min_tokens: 8
  secondary_unmask_ratio: 0.1
  dilate_kernel_size: null
  dilate_iterations: 0
  ratio_schedule:
    type: linear
    start: 0.01
    end: 0.7
    steps: 10000

teacher_student:
  momentum: 0.99
  warmup_steps: 1000
  guided_grad_substitute: true

wandb:
  enabled: true
  project: slot-dino
  entity: null
  run_name: slot_jepa_coco_mask0.7_fixviz
  mode: online

output:
  dir: runs
